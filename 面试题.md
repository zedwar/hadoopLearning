##请描述一下Hadoop的shuffle过程
Hadoop的shuffle过程分为Map端和Reduce端。
Map端：Map端会处理输入数据并产生中间结果，这个中间结果会写到本地磁盘，而不是HDFS。每个Map的输出会先写到内存缓冲区中，当写入的数据达到设定的阈值时，系统将会启动一个线程将缓冲区的数据写到磁盘，这个过程叫做spill。
在spill写入之前，会先进行二次排序，首先根据数据所属的partition进行排序，然后每个partition中的数据再按key来排序。partition的目是将记录划分到不同的Reducer上去，以期望能够达到负载均衡，以后的Reducer就会根据partition来读取自己对应的数据。接着运行combiner(如果设置了的话)，combiner的本质也是一个Reducer，其目的是对将要写入到磁盘上的文件先进行一次处理，这样，写入到磁盘的数据量就会减少。最后将数据写到本地磁盘产生spill文件(spill文件保存在{mapred.local.dir}指定的目录中，Map任务结束后就会被删除)。
最后，每个Map任务可能产生多个spill文件，在每个Map任务完成前，会通过多路归并算法将这些spill文件归并成一个文件。至此，Map的shuffle过程就结束了。
Reduce端：reduceTask 根据自己的分区号，去各个 mapTask 机器上取相应的结果分区文件，reduceTask 会取到同一个分区的来自不同 mapTask 的结果文件，reduceTask 会将这些文件再进行合并（归并排序）。至此，Reduce的shuffle过程就结束了。
##namenode与client和datanode都是通过rpc通信，而client与datanode间通过简单的socket通信
RPC(Remote Procedure Call,远程过程调用)是建立在Socket之上的,出于一种类比的愿望,在一台机器上运行的主程序,可以调用远程另一套机器上的子程序,就像LPC(本地过程调用).
越底层,代码越复杂,灵活性越高,效率越高; 越上层,抽象封装的越好,代码越简单,效率越差. Socket和RPC的区别再次说明了这点.
那么, RPC与Socket通信的区别是什么呢?
RPC是建立在Socket之上的,RPC带来了开发C/S程序的简单可靠的手段,它通过一种叫XDR的数据表达方法描述数据,程序员写伪代码,然后由rpcgen程序翻译为真正的可编译的C语言源代码,再翻译成真正的Client端和Server端程序.
RPC作为普遍的C/S开发方法,开发效率高效,可靠. 但RPC方法的基本原则是--以模块调用的简单性忽略了通讯的具体细节,以便程序员不用关心C/S之间的通讯协议, 集中精力对付实现过程. 这就决定了RPC生成的通讯不可能对每种应用都有恰当的处理方法.  与Socket方法相比,传输相同的有效数据,RPC占用更多的网络带宽.
RPC是在Socket的基础上实现的, 它比socket需要更多的网络和系统资源. 另外, 在对程序优化时, 程序员虽然可以直接修改由rpcgen产生的令人费解的源程序. 但对于追求程序设计高效率的RPC而言, 获得的简单性则被大大削弱.
##hadoop Block Size 是否可以修改
 可以修改
修改方法有两种，
1.修改hdfs块大小的方法
在hdfs-site.xml文件中修改配置块大小的地方，dfs.block.size节点。 
重启集群后，重新上传文件到hadoop集群上，新增的文件会按照新的块大小存储，旧的不会改变。 
2.hadoop指定某个文件的blocksize，而不改变整个集群的blocksize
##简述Hadoop的MR过程
一个InputSplit输入到map，会运行我们实现的Mapper的处理逻辑，对数据进行映射操作。
map输出时，会首先将输出中间结果写入到map自带的环形buffer中（buffer默认大小为100M，可以通过io.sort.mb配置）。
map自带的环形buffer使用容量达到一定门限（默认0.80或80%，可以通过io.sort.spill.percent配置），一个后台线程会准备将buffer中的数据写入到磁盘。
这个后台线程在将buffer中数据写入磁盘之前，会首先将buffer中的数据进行partition（分区，partition数为Reducer的个数），对于每个的数据会基于Key进行一个in-memory排序。
排序后，会检查是否配置了Combiner，如果配置了则直接作用到已排序的每个partition的数据上，对map输出进行化简压缩（这样写入磁盘的数据量就会减少，降低I/O操作开销）。
现在可以将经过处理的buffer中的数据写入磁盘，生成一个文件（每次buffer容量达到设置的门限，都会对应着一个写入到磁盘的文件）。
map任务结束之前，会对输出的多个文件进行合并操作，合并成一个文件（若map输出至少3个文件，在多个文件合并后写入之前，如果配置了Combiner，则会运行来化简压缩输出的数据，文件个数可以通过min.num.splits.for.combine配置；如果指定了压缩map输出，这里会根据配置对数据进行压缩写入磁盘），这个文件仍然保持partition和排序的状态。
reduce阶段，每个reduce任务开始从多个map上拷贝属于自己partition（map阶段已经做好partition，而且每个reduce任务知道应该拷贝哪个partition；拷贝过程是在不同节点之间，Reducer上拷贝线程基于HTTP来通过网络传输数据）。
每个reduce任务拷贝的map任务结果的指定partition，也是先将数据放入到自带的一个buffer中（buffer默认大小为Heap内存的70%，可以通过mapred.job.shuffle.input.buffer.percent配置），如果配置了map结果进行压缩，则这时要先将数据解压缩后放入buffer中。
reduce自带的buffer使用容量达到一定门限（默认0.66或66%，可以通过mapred.job.shuffle.merge.percent配置），或者buffer中存放的map的输出的数量达到一定门限（默认1000，可以通过mapred.inmem.merge.threshold配置），buffer中的数据将会被写入到磁盘中。
在将buffer中多个map输出合并写入磁盘之前，如果设置了Combiner，则会化简压缩合并的map输出。
当属于该reducer的map输出全部拷贝完成，则会在reducer上生成多个文件，这时开始执行合并操作，并保持每个map输出数据中Key的有序性，将多个文件合并成一个文件（在reduce端可能存在buffer和磁盘上都有数据的情况，这样在buffer中的数据可以减少一定量的I/O写入操作开销）。
最后，执行reduce阶段，运行我们实现的Reducer中化简逻辑，最终将结果直接输出到HDFS中（因为Reducer运行在DataNode上，输出结果的第一个replica直接在存储在本地节点上）。 
##Hadoop 默认调度器策略
目前Hadoop有三种比较流行的资源调度器：FIFO 、Capacity Scheduler、Fair Scheduler。目前hadoop2.7默认使用的是Capacity Scheduler容量调度器。
hadoop1.x使用的默认调度器就是FIFO。FIFO采用队列方式将一个一个job任务按照时间先后顺序进行服务。比如排在最前面的job需要若干maptask和若干reducetask，当发现有空闲的服务器节点就分配给这个job，直到job执行完毕。
hadoop2.x使用的默认调度器是Capacity Scheduler。
1、支持多个队列，每个队列可配置一定量的资源，每个采用FIFO的方式调度。
2、为了防止同一个用户的job任务独占队列中的资源，调度器会对同一用户提交的job任务所占资源进行限制。
3、分配新的job任务时，首先计算每个队列中正在运行task个数与其队列应该分配的资源量做比值，然后选择比值最小的队列。比如如图队列A15个task，20%资源量，那么就是15%0.2=70，队列B是25%0.5=50 ，队列C是25%0.3=80.33 。所以选择最小值队列B。
4、其次，按照job任务的优先级和时间顺序，同时要考虑到用户的资源量和内存的限制，对队列中的job任务进行排序执行。
5、多个队列同时按照任务队列内的先后顺序一次执行。例如下图中job11、job21、job31分别在各自队列中顺序比较靠前，三个任务就同时执行。
Fair Scheduler（公平调度器）
1、支持多个队列，每个队列可以配置一定的资源，每个队列中的job任务公平共享其所在队列的所有资源。
2、队列中的job任务都是按照优先级分配资源，优先级越高分配的资源越多，但是为了确保公平每个job任务都会分配到资源。优先级是根据每个job任务的理想获取资源量减去实际获取资源量的差值决定的，差值越大优先级越高。
##Hadoop怎么做到数据保持一致？
HDFS在存储文件将其切分存储时，会对每个块生成副本，副本数根据集群设置不同而有所改变，在默认情况下是为三个。
以默认情况为例，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。
这种存放策略减少了机架间的数据传输，这就提高了写操作的效率。由于机架的错误远远比节点的错误少，所以这个策略不会影响到数据的可靠性和可用性。于此同时，因为数据块只放在两个（不是三个）不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。
总的来说在这种策略下，副本并不是均匀分布在不同的机架上。三分之一的副本在一个节点上，三分之二的副本在一个机架上，其他副本均匀分布在剩下的机架中，这一策略在不损害数据可靠性和读取性能的情况下改进了写的性能。
而数据一致性则是依靠datenode的心跳机制进行保障。
Namenode从所有的 Datanode接收心跳信号和块状态报告。块状态报告包括了某个Datanode所有的数据块列表。每个数据块都有一个指定的最小副本数。当Namenode检测确认某个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全(safely replicated)的；在一定百分比（这个参数可配置）的数据块被Namenode检测确认是安全之后（加上一个额外的30秒等待时间），Namenode将退出
安全模式状态。接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些数据块复制到其他Datanode上。
##以下属于SQL On Hadoop的技术是
Presto是Facebook开发的数据查询引擎，可对250PB以上的数据进行快速地交互式分析。
hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。

##Hadoop HDFS Client端上传文件到HDFS上的时候下列不正确的是
数据经过NameNode传递给DataNode
数据副本将以管道的方式依次传递
Client将数据写到一台DataNode上，并由Client负责完成Block复制工作
当某个DataNode失败，客户端不会继续传给其它的DataNode

A.namenode不具有传输数据的职责，它主要是管理文件系统的命名空间，记录元数据，协调客户端对文件的访问，记录命名空间的改动或空间本身属性的变化
B.这个没有什么争议吧。。。
C.数据块的复制是由datanode负责的，不是client，严格意义上讲，hadoop是没有client的，它只是提供了一个接口来使用
D.说实话我没怎么看懂这个。。。。DataNode失败，客户端不会传给其他DataNode。
##请简单描述下你对hadoop的技术生态圈的了解
HDFS：分布式海量数据存储功能
Yarn：提供资源调度与任务管理功能
资源调度：根据申请的计算任务，合理分配集群中的计算节点(计算机)。
任务管理：任务在执行过程中，负责过程监控、状态反馈、任务再调度等工作。
MapReduce：分布式并行编程模型和计算框架。解决分布式编程门槛高的问题，基于其框架对分布式计算的抽象 map 和 reduce，可以轻松实现分布式计算程序。
Hive：提供数据摘要和查询的数据仓库。解决数据仓库构建问题，基于 Hadoop 平台的存储与计算，与传统 SQL 相结合，让熟悉 SQL 的编程人员轻松向 Hadoop 平台迁移。
Streaming：解决非 Java 开发人员使用 Hadoop 平台的语言问题，使各种语言如 C++、python、 shell 等均可以无障碍使用 Hadoop 平台。
HBase：基于列式存储模型的分布式数据库。解决某些场景下，需要 Hadoop 平台数据及时响应的问题。
Zookeeper：分布式协同服务。主要解决分布式下数据管理问题：统一命名、状态同步、集群管理、配置同步等。
其他的一些开源组件:
1, spark是个开源的数据分析集群计算框架，建立于HDFS之上。spark与hadoop一样，用于构建大规模，延迟低的数据分析应用。spark采用Scala语言实现，使用Scala作为应用框架。spark采用基于内存的分布式数据集，优化了迭代式的工作负载以及交互式查询。
与hadoop不同的是，spark与Scala紧密集成，Scala象管理本地collective对象那样管理分布式数据集。spark支持分布式数据集上的迭代式任务，实际上可以在hadoop文件系统上与hadoop一起运行（通过YARN,MESOS等实现）。
2, kafka是一个开源流处理平台，由Scala和Java编写。Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息
3, Redis是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。
##启用Hadoop2版本的Namenode高可用之后，哪些组件不再使用
Zookeeper
JournalNode
DataNode
SecondaryNameNode
D
##yarn是一种新的hadoop资源管理器，以下哪些开源组件可以运行在hadoop yarn上
1. MapReduce On YARN：YARN天生支持，目前已非常完善（从YARN将要发布2.1.0-beta版可看出，较之前版本，这一块基本没有修改）。
2. Tez On YARN：一个DAG计算框架，直接修改自MapReduce，继承了MapReduce的扩展性好和容错性好等优点
3. Storm On YARN：实时计算框架Storm运行在YARN上，，项目状态：开发进行中，已发布一个版本。
##Yarn中的关键组件并简述各关键组件内部的交互原理
YARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：一个全局的资源管理器ResourceManager和每个应用程序特有的ApplicationMaster。其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。
YARN的基本组成结构，YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等几个组件构成。
ResourceManager是Master上一个独立运行的进程，负责集群统一的资源管理、调度、分配等等；NodeManager是Slave上一个独立运行的进程，负责上报节点的状态；App Master和Container是运行在Slave上的组件，Container是yarn中分配资源的一个单位，包涵内存、CPU等等资源，yarn以Container为单位分配资源。
Client向ResourceManager提交的每一个应用程序都必须有一个Application Master，它经过ResourceManager分配资源后，运行于某一个Slave节点的Container中，具体做事情的Task，同样也运行于某一个Slave节点的Container中。RM，NM，AM乃至普通的Container之间的通信，都是用RPC机制
##Hadoop是离线计算，基于磁盘，每次运算之后的结果需要存储在HDFS里面，下次再用的话，还需要读出来进行一次计算，磁盘IO开销比较大。底层基于HDFS存储文件系统。并且Hadoop只有Map和Reduce两种接口，相对于Spark来说太少了。Spark里面的一个核心的概念就是RDD，弹性分布式数据集。Spark支持内存计算模型，用户可以指定存储的策略，当内存不够的时候，可以放置到磁盘上。并且Spark提供了一组RDD的接口，Transformations和Action。Transformations是把一个RDD转换成为另一个RDD以便形成Lineage血统链，这样当数据发生错误的时候可以快速的依靠这种继承关系恢复数据。Action操作是启动一个Job并开始真正的进行一些计算并把返回的结果可以给Driver或者是缓存在worker里面。
