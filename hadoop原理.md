##Hadoop配置信息处理
###Hadoop Configuration 详解
    Hadoop没有使用java.util.Properties管理配置文件，也没有使用ApacheJakarta Commons Configuration管理配置文件，而是使用了一套
        独有的配置文件管理系统，并提供自己的API，即使用org.apache.hadoop.conf.Configuration处理配置信息
    在Configuration中，每个属性都是String类型的，但是值类型可能是以下多种类型，包括Java中的基本类型，如boolean（getBoolean）、
        int（getInt）、long（getLong）、float（getFloat），也可以是其他类型，如String（get）、java.io.File（getFile）、String
        数组（getStrings）等。以上面的配置文件为例，getInt("io.sort.factor")将返回整数10；而getStrings("dfs.web.ugi")返回一个
        字符串数组，该数组有两个元素，分别是webuser和webgroup。
    Configuration的成员变量
##序列化与压缩
###序列化
    对象的序列化（Serialization）用于将对象编码成一个字节流，以及从字节流中重新构建对象。“将一个对象编码成一个字节流”称为序列化该
        对象（Serializing）；相反的处理过程称为反序列化（Deserializing）        
    序列化有三种主要的用途：作为一种持久化格式：一个对象被序列化以后，它的编码可以被存储到磁盘上，供以后反序列化用。作为一种通信数
        据格式：序列化结果可以从一个正在运行的虚拟机，通过网络被传递到另一个虚拟机上。作为一种拷贝、克隆（clone）机制：将对象序列化
        到内存的缓存区中，然后通过反序列化，可以得到一个对已存对象进行深拷贝的新对象。在分布式数据处理中，主要使用上面提到的前两种
        功能：数据持久化和通信数据格式。
    Java序列化机制将对象转换为连续的byte数据，这些数据可以在日后还原为原先的对象状态，该机制还能自动处理不同操作系统上的差异，在
        Windows系统上序列化的Java对象，可以在UNIX系统上被重建出来，不需要担心不同机器上的数据表示方法，也不需要担心字节排列次序，
        如大端（big endian）、小端（little endian）或其他细节。
    在Java中，使一个类的实例可被序列化非常简单，只需要在类声明中加入implements Serializable即可。Serializable接口是一个标志，不
        具有任何成员函数；Serializable接口没有任何方法，所以不需要对类进行修改，Block类通过声明它实现了Serializable接口，立即可
        以获得Java提供的序列化功能。
    由于序列化主要应用在与I/O相关的一些操作上，其实现是通过一对输入/输出流来实现的。如果想对某个对象执行序列化动作，可以在某种
        OutputStream对象（后面还会讨论Java的流）的基础上创建一个对象流ObjectOutputStream对象，然后调用writeObject()就可达到目
        的。writeObject()方法负责写入实现了Serializable接口对象的状态信息，输出数据将被送至该OutputStream。多个对象的序列化可以
        在ObjectOutputStream对象上多次调用writeObject()，分别写入这些对象。
    对于Java基本类型的序列化，ObjectOutputStream提供了writeBoolean()、writeByte()等方法
    Java的序列化机制非常“聪明”，但是，序列化以后的对象在尺寸上有点过于充实了
    和Java序列化机制不同（在对象流ObjectOutputStream对象上调用writeObject()方法），Hadoop的序列化机制通过调用对象的write()方法
        （它带有一个类型为DataOutput的参数），将对象序列化到流中。反序列化的过程也是类似，通过对象的readFields()，从流中读取数据。
        值得一提的是，Java序列化机制中，反序列化过程会不断地创建新的对象，但在Hadoop的序列化机制的反序列化过程中，用户可以复用对
        象：如，在Block的某个对象上反复调用readFields()，可以在同一个对象上得到多个反序列化的结果，而不是多个反序列化的结果对象
        （对象被复用了），这减少了Java对象的分配和回收，提高了应用的效率。
    对于处理大规模数据的Hadoop平台，其序列化机制需要具有如下特征：紧凑：由于带宽是Hadoop集群中最稀缺的资源，一个紧凑的序列化机制
        可以充分利用数据中心的带宽。快速：在进程间通信（包括MapReduce过程中涉及的数据交互）时会大量使用序列化机制，因此，必须尽量
        减少序列化和反序列化的开销。可扩展：随着系统的发展，系统间通信的协议会升级，类的定义会发生变化，序列化机制需要支持这些升级
        和变化。互操作：可以支持不同开发语言间的通信，如C++和Java间的通信。这样的通信，可以通过文件（需要精心设计文件的格式）或者
        后面介绍的IPC机制实现。
    为了支持以上这些特性，Hadoop引入org.apache.hadoop.io.Writable接口，作为所有可序列化对象必须实现的接口
    Writable.write()方法用于将对象状态写入二进制的DataOutput中，反序列化的过程由readFields()从DataInput流中读取状态完成。
    Hadoop序列化机制中还包括另外几个重要接口：WritableComparable、RawComparator和WritableComparator。WritableComparable，顾名
        思义，它提供类型比较的能力，这对MapReduce至关重要。该接口继承自Writable接口和Comparable接口，其中Comparable用于进行类型
        比较。ByteWritable、IntWritable、DoubleWritable等Java基本类型对应的Writable类型，都继承自WritableComparable。
    RawComparator接口允许执行者比较流中读取的未被反序列化为对象的记录，从而省去了创建对象的所有开销。
    固定长度格式的整型，序列化后的数据是定长的，而可变长度格式则使用一种比较灵活的编码方式，对于数值比较小的整型，它们往往比较节省空间。
    除了前面介绍过的Java序列化机制和Hadoop使用的Writable机制，还流行其他序列化框架，如Hadoop Avro、Apache Thrift和GoogleProtocol Buffer。
    □Avro是一个数据序列化系统，用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷、快速地处理大量数据；
        动态语言友好，Avro提供的机制使动态语言可以方便地处理Avro数据。□Thrift是一个可伸缩的、跨语言的服务开发框架，由Facebook贡
        献给开源社区，是Facebook的核心框架之一。基于Thrift的跨平台能力封装的Hadoop文件系统Thrift API（参考contrib的thriftfs模块），
        提供了不同开发语言开发的系统访问HDFS的能力。□Google Protocol Buffer是Google内部的混合语言数据标准，提供了一种轻便高效的
        结构化数据存储格式。目前，Protocol Buffers提供了C++、Java、Python三种语言的API，广泛应用于Google内部的通信协议、数据存
        储等领域中。
    Hadoop提供了一个简单的序列化框架API，用于集成各种序列化实现，该框架由Serialization实现（在org.apache.hadoop.io.serializer包中）
    Serialization是一个接口，使用抽象工厂的设计模式，提供了一系列和序列化相关并相互依赖对象的接口。通过Serialization应用可以获得
        类型的Serializer实例，即将一个对象转换为一个字节流的实现实例；Deserializer实例和Serializer实例相反，它用于将字节流转为一
        个对象。很明显，Serializer和Deserializer相互依赖，所以必须通过抽象工厂Serialization，才能获得对应的实现。
    如果需要使用Serializer来执行序列化，一般需要通过open()方法打开Serializer，open()方法传入一个底层的流对象，然后就可以使用
        serialize()方法序列化对象到底层的流中。最后序列化结束时，通过close()方法关闭Serializer
###压缩
    计算机处理的数据都存在一些冗余度，同时数据中间，尤其是相邻数据间存在着相关性，所以可以通过一些有别于原始编码的特殊编码方式来保
        存数据，使数据占用的存储空间比较小，这个过程一般叫压缩    
    压缩广泛应用于海量数据处理中，对数据文件进行压缩，可以有效减少存储文件所需的空间，并加快数据在网络上或者到磁盘上的传输速度。
    数据压缩的方式非常多，不同特点的数据有不同的数据压缩方式：如对声音和图像等特殊数据的压缩，就可以采用有损的压缩方法，允许压缩过
        程中损失一定的信息，换取比较大的压缩比；而对音乐数据的压缩，由于数据有自己比较特殊的编码方式，因此也可以采用一些针对这些特
        殊编码的专用数据压缩算法。
    Hadoop作为一个较通用的海量数据处理平台，在使用压缩方式方面，主要考虑压缩速度和压缩文件的可分割性。
    所有的压缩算法都会考虑时间和空间的权衡，更快的压缩和解压缩速度通常会耗费更多的空间（压缩比较低）。例如，通过gzip命令压缩数据
        时，用户可以设置不同的选项来选择速度优先或空间优先，选项–1表示优先考虑速度，选项–9表示空间最优，可以获得最大的压缩比。需
        要注意的是，有些压缩算法的压缩和解压缩速度会有比较大的差别：gzip和zip是通用的压缩工具，在时间/空间处理上相对平衡，gzip2压
        缩比gzip和zip更有效，但速度较慢，而且bzip2的解压缩速度快于它的压缩速度。
    当使用MapReduce处理压缩文件时，需要考虑压缩文件的可分割性。考虑我们需要对保持在HDFS上的一个大小为1GB的文本文件进行处理，当前
        HDFS的数据块大小为64MB的情况下，该文件被存储为16块，对应的MapReduce作业将会将该文件分为16个输入分片，提供给16个独立的Map
        任务进行处理。但如果该文件是一个gzip格式的压缩文件（大小不变），这时，MapReduce作业不能够将该文件分为16个分片，因为不可能
        从gzip数据流中的某个点开始，进行数据解压。但是，如果该文件是一个bzip2格式的压缩文件，那么，MapReduce作业可以通过bzip2格式
        压缩文件中的块，将输入划分为若干输入分片，并从块开始处开始解压缩数据。bzip2格式压缩文件中，块与块间提供了一个48位的同步标
        记，因此，bzip2支持数据分割。
    Hadoop通过以编码/解码器为基础的抽象工厂方法，提供了一个可扩展的框架，支持多种压缩方法。下面就来研究Hadoop压缩框架的实现。
    1.编码/解码器前面已经提过，CompressionCodec接口实现了编码/解码器，使用的是抽象工厂的设计模式。CompressionCodec提供了一系列
        方法，用于创建特定压缩算法的相关设施，其类图如图3-5所示。CompressionCodec中的方法很对称，一个压缩功能总对应着一个解压缩
        功能。其中，与压缩有关的方法包括：createOutputStream()用于通过底层输出流创建对应压缩算法的压缩流，重载的createOutputStream()
        方法可使用压缩器创建压缩流；createCompressor()方法用于创建压缩算法对应的压缩器。后续会继续介绍压缩流CompressionOutputStream
        和压缩器Compressor。解压缩也有对应的方法和类。
    抽象工厂方法和工厂方法这两个设计模式有很大的区别，抽象工厂方法用于创建一系列相关或互相依赖的对象，如CompressionCodec可以获得
        和某一个压缩算法相关的对象，包括压缩流和解压缩流等。而工厂方法（严格来说，CompressionCodecFactory是参数化工厂方法），用
        于创建多种产品，如通过CompressionCodecFactory的getCodec()方法，可以创建GzipCodec对象或BZip2Codec对象。
    压缩器（Compressor）和解压器（Decompressor）是Hadoop压缩框架中的一对重要概念。Compressor可以插入压缩输出流的实现中，提供具
        体的压缩功能；相反，Decompressor提供具体的解压功能并插入CompressionInputStream中。Compressor和Decompressor的这种设计，
        最初是在Java的zlib压缩程序库中引入的，对应的实现分别是java.util.zip.Deflater和java.util.zip.Inflater。下面以Compressor
        为例介绍这对组件。Compressor的用法相对复杂，请参考org.hadoopinternal.compress.CompressDemo的compressor()方法。Compressor
        通过setInput()方法接收数据到内部缓冲区，自然可以多次调用setInput()方法，但内部缓冲区总是会被写满。如何判断压缩器内部缓冲
        区是否已满呢？可以通过needsInput()的返回值，如果是false，表明缓冲区已经满，这时必须通过compress()方法获取压缩后的数据，
        释放缓冲区空间。为了提高压缩效率，并不是每次用户调用setInput()方法，压缩器就会立即工作，所以，为了通知压缩器所有数据已经
        写入，必须使用finish()方法。finish()调用结束后，压缩器缓冲区中保持的已经压缩的数据，可以继续通过compress()方法获得。至于
        要判断压缩器中是否还有未读取的压缩数据，则需要利用finished()方法来判断。
    Java最初版本的输入/输出系统是基于流的，流抽象了任何有能力产出数据的数据源，或者是有能力接收数据的接收端。一般来说，通过设计模
        式装饰，可以为流添加一些额外的功能，如前面提及的序列化流ObjectInputStream和ObjectOutputStream。压缩流（CompressionOutputStream）
        和解压缩流（CompressionInputStream）是Hadoop压缩框架中的另一对重要概念，它提供了基于流的压缩解压缩能力。如图3-7所示是从
        java.io.InputStream和java.io.OutputStream开始的类图。这里只分析和压缩相关的代码，即CompressionOutputStream及其子类。
        OutputStream是一个抽象类，提供了进行流输出的基本方法，它包含三个write成员函数，分别用于往流中写入一个字节、一个字节数组
        或一个字节数组的一部分（需要提供起始偏移量和长度）。
    数据压缩往往是计算密集型的操作，考虑到性能，建议使用本地库（NativeLibrary）来压缩和解压。在某个测试中，与Java实现的内置gzip压
        缩相比，使用本地gzip压缩库可以将解压时间减少50%，而压缩时间大概减少10%。Hadoop的DEFLATE、gzip和Snappy都支持算法的本地实
        现，其中Apache发行版中还包含了DEFLATE和gzip的32位和64位Linux本地压缩库（Cloudera发行版还包括Snappy压缩方法）。默认情况
        下，Hadoop会在它运行的平台上查找本地库。
    实现了SnappyCompressor.c以后，作为本地方法开发的后续步骤，还需要使用C的编译器、链接器编译相关的文件并连接成动态库（Cloudera
        发行版中，SnappyCompressor.c和其他本地库打包成libhadoop.so，当然，运行时还需要将Snappy的动态库加载进来）。在运行时，需
        要将包含该动态库的路径放入系统属性java.library.path中，这样，Java虚拟机才能找到对应的动态库。
    SnappyCompressor的主要属性有compressedDirectBuf和uncompressedDirectBuf，分别用于保存压缩前后的数据，类型都是Buffer。缓冲
        区Buffer代表一个有限容量的容器，是Java NIO（新输入/输出系统）中的重要概念，和基于流的Java IO不同，缓冲区可以用于输入，也
        可以用于输出。为了支持这些特性，缓冲区会维持一些标记，记录目前缓冲区中的数据存放情况（第4章详细介绍Buffer，读者可以参考JavaNIO的内容）。
    
##第四章Hadoop远程过程调用
    作为典型的分布式系统，Hadoop中各个实体间存在着大量的交互，远程过程调用让用户可以像调用本地方法一样调用另外一个应用程序提供的
        服务，而不必设计和开发相关的信息发送、处理和接收等具体代码，是一种重要的分布式计算技术，它提高了程序的互操作性，在Hadoop
        的实现中得到广泛的应用。    
    远程过程调用（Remote Procedure Call，RPC）由Birrell和Nelson于1984年引入分布式计算领域，是解决上述访问透明性的一套精妙方案。
        随着RPC的发展，出现了大量的相关技术，如XML-RPC、JSON-RPC（JavaScript Object NotationRPC），CORBA（Common ObjectRequ
        est Broker Architecture，公共对象请求代理体系结构）和RMI（Remote Method Invocation，远程方法调用）等。
    RPC就是允许程序调用位于其他机器上的过程（也可以是同一台机器的不同进程）。当机器A上的进程调用机器B上的进程时，A上的调用进程被
        挂起，而B上的被调用进程开始执行。调用方使用参数将信息传送给被调用方，然后通过传回的结果得到信息。在这个过程中，A是RPC客户，
        B是RPC服务器。同时，编程人员看不到任何消息的传递，其行为如同一个过程到另一个过程的调用一样。
    传统的过程调用中，主程序将参数压入栈内并调用过程，这时候主程序停止执行并开始执行相应的过程。被调用的过程从栈中获取参数，然后执
        行过程函数；执行完毕后，将返回参数入栈（或者保存在寄存器里），并将控制权交还给调用方。调用方获取返回参数，并继续执行。
    RPC跨越了不同的进程（theClient和theServer），其调用示例如图4-2所示。当theClient需要调用一个远程过程的时候，通过theClient将
        参数打包成为一个消息，并附加被调用过程的名字（指明要调用服务器上的哪一个过程），然后发送消息到服务器。theClient发送完消息
        以后，必须等待服务器的应答，这个时候，执行流是空闲的。
    RPC的Server运行时会阻塞在接收消息的调用上，当接到客户端的请求后，它会解包以获取请求参数，这类似传统过程调用中，被调用函数从栈
        中接收参数，然后确定调用过程的名字并调用相应过程。调用结束后，返回值通过主程序打包并发送回客户端，通知客户端调用结束。
    RPC机制使两个进程间能够使用传统过程调用中的控制流模型来进行交互。RPC看似简单，但在实现的时候，存在很多的具体问题，例如如何在
        系统有差异的情况下进行参数和结果的传递，如果通信双方的某一个机器崩溃了，该如何解决等。为了保证RPC具有与本地调用相应的形式，
        实现RPC需要关注下面三个问题：RPC的语法应该与高级程序设计语言中的本地过程调用的语法有相同的外观；RPC和本地调用的语义尽可能
        地类似，如参数传递中包含的语义；RPC的接收者（也就是服务器上业务代码的执行）应该如同一个传统的调用。
    RPC引入客户存根（Client Stub）和服务器骨架（Server Skeleton）解决了前面提及的问题。
    客户端应用使用相同的方式调用普通方法localF和远程方法remoteF。也就是说，都需要进行相同的参数入栈，并执行对应的子程序。唯一不同
        的是，remoteF的客户存根只是简单地将参数打包成一个消息，并请求将此消息发送到服务器。然后，存根通过调用receive过程，随即阻
        塞自己，直到收到响应消息。另一端的服务器骨架是客户存根的等价物，它也是一段代码，用来将通过网络输入的请求转换为本地调用。服
        务器骨架先调用receive，然后阻塞，等待消息输入。收到消息后，服务器骨架将参数从消息中提取出来，然后以常规方式调用服务器上的
        相应过程，即图中的remoteF过程。从remoteF的角度看，过程就好像是由普通客户直接调用一样，参数和返回地址都位于栈中。remoteF执
        行完成，结果返回到服务器骨架，由骨架打包成消息发送（调用send）给客户存根。服务器骨架再次调用receive，等待下一条消息。客户
        端的receive随着消息的到达而返回，客户存根检查消息并解包返回参数，通过return将结果传递到主程序中。调用者在remoteF结束后重
        新获得控制权，它唯一知道的就是已经得到了所要的数据，它并不清楚操作是通过本地方法进行的，还是远程完成的。
    RPC通过接口定义语言（Interface Definition Language，IDL）描述调用接口的相关信息。IDL文件可以包含类型定义、常量声明、进行参
        数传递时需要的一些其他信息和注释。IDL文件编写完以后，通过调用IDL编译器编译，一般会输出以下文件：头文件（C语言，包括一些函
        数的定义）客户存根服务器骨架应用开发人员编写客户端和服务器代码，编译、连接（客户代码和客户存根目标文件和运行库连接，服务器
        代码和服务器骨架目标文件和运行库连接），就可以得到客户程序和服务器程序，完成一个包含RPC的应用程序的开发。
    Java远程方法调用（Remote Method Invocation，RMI）是Java的一个核心API和类库，允许一个Java虚拟机上运行的Java程序调用不同虚拟
        机上运行的对象中的方法，即使这两个虚拟机运行于物理隔离的不同主机上。在某种程度上，RMI可以看成RPC的Java升级版。
    和RPC一样，包含RMI的Java应用程序通常包括服务器程序和客户端程序。典型的服务器应用程序将创建多个远程对象（Remote Object），使
        这些对象能够被客户端引用，并等待客户端调用远程对象的方法；而典型的客户端程序则从服务器中得到一个或多个远程对象的引用，然
        后调用位于服务器端的远程对象方法。在这个过程中，RMI提供了和RPC中类似的、标准的Stub/Skeleton机制。Java RMI和RPC工作原理
        非常类似，Stub代表可以被客户端引用的远程对象，位于客户端，并保存着远程对象的接口和方法列表。客户端应用调用远程对象时，Stub
        将调用请求，通过RMI的基础结构转发到远程对象上。接收到调用请求时，服务器端的Skeleton对象处理有关调用“远方”对象中的所有细节，
        并调用Skeleton对象。
    远程对象是指这样一类对象：除了对象本身所在的虚拟机，其他虚拟机也可以调用此对象的方法，而且这些虚拟机一般运行在不同的计算机上。
        每个远程对象都实现了一个或多个远程接口（Remote Interface），远程接口声明了可以由外部系统调用的远程对象方法。
    远程接口必须声明为public，否则客户端试着装载“实现远程接口”的远端对象时，会收到错误的消息。远程接口必须继承自java.rmi.Remote。
        远程接口中的每一个方法，除了自定义的异常之外，必须将java.rmi.RemoteException声明于其throws子句中。在远程方法声明中，作
        为参数或者返回值的远程对象，或者包含在其他非远程对象中的远程对象，必须声明为其对应的远程接口，而不是实际的实现类。
    调用远程方法和调用本地方法有着近乎相同的“感观”，一个主要的差别在于需要通过命名服务获得远程对象的引用；另外，和本地方法调用不一
        样，远程方法可能抛出RemoteException异常，客户端程序需要处理该异常。
    客户端RMIQueryStatusClient的工作依赖于RMI存根，这个存根是通过Java的代理机制java.lang.reflect.Proxy（下一节介绍）动态生成的。
        存根包含了Remote接口的信息，在例子中，这个信息就是：远程对象RMIQueryStatusImpl中提供了远程方法getFileStatus()。在一些
        特殊情况下，开发人员需要自己生成存根，与RPC提供IDL和IDL编译器类似，JDK提供rmic工具辅助用户开发。该工具可以根据RMIQueryStatusImpl
        的字节码生成相应的存根：RMIQueryStatusImpl_Stub.class。
    Java动态代理中大量使用了Java接口，与RMI的例子类似，dynamicproxy中使用PDQueryStatus接口，它包含一个非常简单的getFileStatus()
        方法，该方法接受一个字符串参数，返回一个类型为DPFileStatus的Java对象。和远程方法调用不同，对PDQueryStatus接口，Java动态
        代理机制没有额外的要求。
    Java的最初版本中，只提供了基于流的基本套接字（socket）实现，socket是两台主机间的一个连接，可以进行7项基本操作：连接远程机器。
        发送数据。接收数据。关闭连接。绑定端口。监听入站数据。在所绑定端口上接受来自远程机器的连接。其中，前四项用于客户端，后六项
        用于服务器，最后三项只有服务器才需要，即等待客户端的连接，这些操作通过ServerSocket类实现。
    Java程序一般以下面的顺序使用客户端socket：1）通过构造函数构造一个新Socket对象；2）将socket连接到远程主机；3）连接建立以后，
        本地和远程主机从socket取得输出流和输入流，使用这两个流彼此发送数据，两台主机都可以同时发送和接收数据；4）数据传输完毕后，
        关闭连接。
    当我们需要实现同时处理上千个客户请求的服务器时，Java基本套接字会产生一些问题：由于OutputStream的write()方法、InputStream的
        read()方法和ServerSocket的accept()方法[插图]都是阻塞方法，往往需要采用一个客户对应一个线程的服务器系统，虽然使用线程池
        在某种程度上能够节省部分系统开销，但对于生存期很长的协议来说，大量的闲置客户端限制了系统可以同时服务的客户端总数。为了解
        决这样的问题，JDK 1.4中引入了Java新输入/输出系统（New Input/Output，NIO）
    非阻塞是NIO实现的重要功能之一，为了实现非阻塞，NIO引入了选择器（Selector）和通道（Channel）的概念。通道表示到实体，如硬件设
        备、文件、网络套接字或可以执行一个或多个不同I/O操作（如读取或写入）的程序组件的开放连接。一些通道，如文件、网络套接字，允
        许选择器对通道进行轮询。也就是说，通道能够注册一个选择器实例，通过该实例的select()方法，用户可以询问“在（一个或一组）通道
        中，哪一个是当前需要的服务（即被读、写或接受）”。在一个准备好的通道上进行相应的I/O操作，就不需要等待，也就不会阻塞了。
    NIO中一个主要的特性是java.nio.Buffer。缓冲区（Buffer）提供了一个比流抽象的、更高效和可预测的I/O。Buffer代表了一个有限容量的
        容器—其本质是一个数组，通道Channel使用Buffer实例来传递数据。
    从编程的角度看，流和通道的差别在于流是基于字节的，而通道是基于块的（也就是Buffer）。流设计为一个字节接着一个字节、按顺序提供
        数据，虽然基于性能考虑，流也可以传输字节数组，但最基本的流概念要求一次传送一个字节。通道不同，通道使用的是缓冲区中的数据
        块，即Buffer，在读写通道数据时，这些数据必须存储在缓冲区中，一次读写一个缓冲区的数据。
    capacity：缓冲区的元素总数（容量），通过Buffer.capacity()可以获取该索引。缓冲区元素总数是不可修改的。position：缓冲区的位置，
        是下一个要读取或写入的元素的索引，该位置可以由position()方法和position(int)方法获取和设置。limit：缓冲区的限制，即第一个
        不应该读取或写入的元素的索引，limit()方法和limit(int)方法可用于获取和设置缓冲区的限制。mark：缓冲区的位置设置标记，通过mark()
        方法设置一个位置，然后利用reset()方法可以将position重置为mark()方法设置的位置。
    假设某缓冲区目前处于某状态，这个时候position不等于0，limit也不等于capacity；如果现在想将数据读入/写入到这个缓冲区中，首先要
        先对该缓冲区进行清空，通过clear()方法，缓冲区的poistion被设置为0，limit设置为capacity，这样，缓冲区准备好接收新数据。后
        续的put()/read()调用，将数据从第一个元素开始填入缓冲区，最多直到填满该缓冲区，达到limit位置（等于capacity）。
    IPCQueryStatus接口定义了IPC服务器对外提供的功能，它使用IPCFileStatus作为getFileStatus()方法的返回结果；IPCQueryStatusImpl
        实现了接口中定义的功能，IPCQueryServer利用RMIQueryStatusImpl的实例，构造并运行IPC服务器；IPCQueryStatusClient作为客户
        端，使用IPC访问服务器提供的RMIQueryStatus功能。Java RMI的开发从远程接口的定义开始，远程接口必须继承自java.rmi.Remote；
        在Hadoop远程过程调用中，也是通过一个IPC接口开始进行开发。Hadoop IPC接口必须继承自org.apache.hadoop.ipc.VersionedProtocol接口
    RPC.getProxy()用于获得一个IPC客户端接口实例，它需要4个参数：□IPC接口的类对象：可以通过IPC接口的静态成员class直接获得。接口的
        静态成员class保存了该接口的java.lang.Class实例，java.lang.Class类的实例表示正在运行的Java应用程序中的类和接口，提供了一
        系列与Java反射相关的重要功能。□接口版本：在Hadoop这样复杂的系统中，接口会根据需求不断地进行升级，形成多个版本的IPC接口。
        如果IPC客户端和服务器使用同一个IPC接口的不同版本进行通信，结果将是灾难性的，所以在建立IPC时，需要对IPC的双方进行版本检查。
        □服务器的Socket地址：用于建立IPC的底层TCP连接。□配置类Configuration对象：用于定制IPC客户端参数。
    连接关闭操作的原因包括：服务器负载太高，出现OutOfMemoryError异常；长时间没有收到客户端发送过来的心跳消息；服务器响应在很长时
        间内不能发送给客户端；客户端主动关闭连接；处理服务器响应时出错；服务器关闭。
    代理（Proxy）在《现代汉语词典》里的解释是：受当事人委托，代表他进行某种活动，如贸易、诉讼、纳税、签订合同等。如果把这个解释应
        用到面向对象领域，应该是：代理对象，代表另一个对象（目标对象，Target），执行相关活动。也就是说，在实际应用中使用代理对象
        作为中介，代替目标对象。
    ###Hadoop IPC连接相关过程
    IPC连接是客户端和服务器关系的一个抽象，它的实现包括连接建立、连接上的数据读写、连接维护和连接关闭四个流程。
    IPC方法调用，在客户端只是一个比较复杂的Java动态代理应用；在服务器端，由Listener、Handler和Responder配合，完成请求读取、请求处理和请求应答三个步骤。
##第五章Hadoop文件系统
    ###文件系统的实现
    文件的物理结构指文件在存储设备（如磁带、磁盘、光盘）上的存取方式。为了便于管理，设备往往将存储空间组织成为具有一定结构的存储单位。
    Linux的本地文件系统，i-node或索引节点是Linux/UNIX文件系统中最有名也是最重要的概念，它存储了文件和目录的元数据（HDFS中对应的结
        构分别是INodeFile和INodeDirectory，它们是INode的子类，借鉴了i-node的命名）。所有索引节点大小相同都是128字节，这样，如果
        磁盘的块（块由多个连续的扇区组成）大小为1024字节，它可以包含8个索引节点。
    对于文件的读、写和执行权限，具体内容为：r（read）：可以读取文件的内容。w（write）：可以编辑、修改文件的内容。x（execute）：该
        文件可以被执行。需要注意的是，文件的这些权限都是针对文件的内容，与文件本身没有任何关系。即便是对文件有“rwx”权限，用户也不一
        定可以修改文件名或删除文件。
    对于目录，如果将目录下的所有文件/子目录看成是目录的内容，其读、写和执行权限具体内容为：r（read）：可以读取文件夹内容列表，但如
        果用户没有“x”权限，就只能看到文件名而无法查看其他内容（大小、权限等）。w（write）：用户具有“w”权限，就可以修改目录的内容，
        即文件夹记录列表，前提是用户拥有“x”权限，可以进入这个目录。修改目录的内容，也就是“w”权限，包括：建立新的文件或文件夹、删
        除已存在的文件或文件夹、对已存在的文件或文件夹改名和更改目录内文件或文件夹的位置。x（execute）：可以进入该文件夹，没有“x”权
        限便无法执行该目录下的任何命令。需要注意的是，对文件/子目录的改名与删除，系统检查文件父目录的“w”权限，而和文件本身的权限没
        有关系。当用户改名或删除文件/子目录时，执行的是对它上一级目录的“w”操作，因此，删除文件/子目录不需要考虑被删除文件/子目录自
        身的权限设置。
    NFS背后的基本思想是：每个文件服务器都提供其本地文件系统的一个标准化视图。换句话说，无论本地文件系统是如何实现的，每个NFS服务器
        通过一个通信协议，支持相同的模型，该协议允许客户以完全相同的方法访问存储在一个服务器上的文件，从而允许客户进程共享公用的文
        件系统。用于UNIX系统的基本NFS体系结构如下图。其中，VFS是虚拟文件系统，NFS客户端作为VFS的一个具体的文件系统。当应用程序对
        文件进行操作时，内核处理完与文件系统无关的操作后，调用NFS客户，该组件负责处理对存储在远程服务器上的文件的访问。
    URI（Uniform Resource Identifier，统一资源标识符）以特定的语法标识一个资源的字符串。所标识的资源可以是文件，如上面File类的
        例子，也可以是一个电子邮件地址、一本书或其他东西。绝对URI由URI模式和模式特有部分组成，它们之间由冒号隔开
    URL用于标识Internet上资源的位置，可以指定用于访问服务器的协议（如FTP、HTTP）、服务器的名称和文件在此服务器上的位置。典型的URL，
        如上面提到的“http：//www.ietf.org/rfc/rfc2396.txt”指明了服务器www.ietf.org的目录rfc下有一个名字为“rfc2396.txt”的文件，
        此文件可以通过HTTP协议进行访问。
    Hadoop借鉴了Linux虚拟文件系统的概念，引入了Hadoop抽象文件系统，并在Hadoop抽象文件系统的基础上，提供了大量的具体文件系统的实
        现，满足构建于Hadoop上应用的各种数据访问需求。
    HarFileSystem：保存大量的（小）文件会消耗HDFS名字节点的大量内存，并对HDFS的性能造成影响。Hadoop Archives或HAR文件系统，通过
        将（小）文件归档（类似于UNIX系统中的归档命令tar），形成一些大文件，更高效地将大量（小）文件存放在原始文件系统中（注意，HAR
        文件也可以保存在其他基础文件系统中）。Hadoop Archives将文件的目录信息和文件数据的起始位置、长度保存在索引文件中。索引文件
        和归档存放的数据一起，保存在原始文件系统上。HarFileSystem实现了Hadoop抽象文件系统，使得构建于Hadoop MapReduce的应用可以
        访问Hadoop归档文件，处理大量的（小）文件输入。
##Hadoop分布式文件系统
    HDFS的主要特性包括：□支持超大文件。超大文件在这里指的是几百MB、几百GB甚至几TB大小的文件，一般来说，一个Hadoop文件系统会存储T
        （1TB=1024GB）、P（1P=1024T）级别的数据。Hadoop需要能够支持这种级别的大文件。□检测和快速应对硬件故障。在大量通用硬件平台
        上构建集群时，故障，特别是硬件故障是常见的问题。一般的HDFS系统是由数百台甚至上千台存储着数据文件的服务器组成，这么多的服务
        器意味着高故障率。因此，故障检测和自动恢复是HDFS的一个设计目标。□流式数据访问。HDFS处理的数据规模都比较大，应用一次需要访问
        大量的数据。同时，这些应用一般是批量处理，而不是用户交互式处理。HDFS使应用程序能够以流的形式访问数据集，注重的是数据的吞吐
        量，而不是数据访问的速度。□简化的一致性模型。大部分的HDFS程序操作文件时需要一次写入，多次读取。在HDFS中，一个文件一旦经过
        创建、写入、关闭后，一般就不需要修改了。这样简单的一致性模型，有利于提供高吞吐量的数据访问模型。
##第七章数据节点实现
    其中各个目录的作用如下：blocksBeingWritten：由名字可以知道，该文件夹保存着当前正在“写”的数据块。和位于“tmp”目录下保存的正在
        “写”的数据块相比，差别在于“blocksBeingWritten”中的数据块写操作由客户端发起。current：数据节点管理的最重要目录，它保存着
        已经写入HDFS文件系统的数据块，也就是写操作已经结束的已“提交”数据块。该目录还包含一些系统工作时需要的文件。detach：用于配
        合数据节点升级，供数据块分离操作保存临时工作文件。tmp：该文件夹也保存着当前正在“写”的数据块，这里的写操作是由数据块复制引
        发的，另一个数据节点正在发送数据到数据块中。   
     
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    