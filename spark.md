##第一章简介
Spark实现了一种分布式的内存抽象，称为弹性分布式数据集（ResilientDistributed Dataset，RDD）。它支持基于工作集的应用，同时具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。
RDD提供了一种高度受限的共享内存模型，即RDD是只读记录分区的集合，只能通过在其他RDD执行确定的转换操作（如map、join和groupBy）而创建，然而这些限制使得实现容错的开销很低。与分布式共享内存系统需要付出高昂代价的检查点和回滚机制不同，RDD通过Lineage来重建丢失的分区：一个RDD中包含了如何从其他RDD衍生所必需的相关信息，从而不需要检查点操作就可以重建丢失的数据分区。尽管RDD不是一个通用的共享内存抽象，但它具备了良好的描述能力、可伸缩性和可靠性，能够广泛适用于数据并行类应用。
spark优点
第一个优点是速度。与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上（如图1-1所示）；而基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流
第二个优点就是易用。Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，这意味着可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法，而不是像以前一样，需要打包、上传集群、验证等。这对于原型开发非常重要。
第三个优点是通用性。Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（通过Spark SQL）、实时流处理（通过SparkStreaming）、机器学习（通过Spark MLlib）和图计算（通过SparkGraphX），如图1-2所示。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。当然还有，作为统一的解决方案，Spark并没有以牺牲性能为代价。相反，在性能方面，Spark具有很大的优势。
第四个优点就是可融合性。Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。
Spark的整体架构如图1-3所示。其中，Driver是用户编写的数据处理逻辑，这个逻辑中包含用户创建的SparkContext。SparkContext是用户逻辑与Spark集群主要的交互接口，它会和Cluster Manager交互，包括向它申请计算资源等。Cluster Manager负责集群的资源管理和调度，现在支持Standalone、Apache Mesos和Hadoop的YARN。Worker Node是集群中可以执行计算任务的节点。Executor 是在一个Worker Node上为某应用启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上。Task是被送到某个Executor上的计算单元。每个应用都有各自独立的Executor，计算最终在计算节点的Executor中执行。
用户程序从最开始的提交到最终的计算执行，需要经历以下几个阶段：1）用户程序创建SparkContext时，新创建的SparkContext实例会连接到Cluster Manager。Cluster Manager会根据用户提交时设置的CPU和内存等信息为本次提交分配计算资源，启动Executor。2）Driver会将用户程序划分为不同的执行阶段，每个执行阶段由一组完全相同的Task组成，这些Task分别作用于待处理数据的不同分区。在阶段划分完成和Task创建后，Driver会向Executor发送Task。3）Executor在接收到Task后，会下载Task的运行时依赖，在准备好Task的执行环境后，会开始执行Task，并且将Task的运行状态汇报给Driver。4）Driver会根据收到的Task的运行状态来处理不同的状态更新。Task分为两种：一种是Shuffle Map Task，它实现数据的重新洗牌，洗牌的结果保存到Executor所在节点的文件系统中；另外一种是Result Task，它负责生成结果数据。5）Driver会不断地调用Task，将Task发送到Executor执行，在所有的Task都正确执行或者超过执行次数的限制仍然没有执行成功时停止。
Spark Streaming基于Spark Core实现了可扩展、高吞吐和容错的实时数据流处理。现在支持的数据源有Kafka、Flume、Twitter、ZeroMQ、Kinesis、HDFS、S3和TCP socket。处理后的结果可以存储到HDFS、Database或者Dashboard中
Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark，也就是把Spark Streaming的输入数据按照批处理尺寸（如1秒）分成一段一段的数据（Stream），每一段数据都转换成Spark中的RDD，然后将Spark Streaming中对DStream的转换操作变为针对Spark中对RDD的转换操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算可以根据业务的需求对中间的结果进行叠加，或者存储到外部设备。
Spark Streaming提供了一套高效、可容错的准实时大规模流式处理框架，它能和批处理及即时查询放在同一个软件栈中，降低学习成本。对于熟悉Spark编程的用户，可以用较低的成本学习Spark Streaming编程。
MLlib是Spark对常用的机器学习算法的实现库，同时含有相关的测试和数据生成器，包括分类、回归、聚类、协同过滤、降维（dimensionalityreduction）以及底层基本的优化元素。
在Spark 1.2.0中，MLlib最大的改进是引入了称为spark.ml的机器学习工具包，支持了流水线的学习模式，即多个算法可以用不同参数以流水线的形式运行。在工业界的机器学习应用部署过程中，流水线的工作模式是很常见的。新的ML工具包使用Spark的SchemaRDD来表示机器学习的数据集合，提供了Spark SQL直接访问的接口。此外，在机器学习的算法方面，增加了两种基于树的方法，即随机森林和梯度增强树。
现在，MLlib实现了许多常用的算法，与分类和回归相关的算法包括SVM、逻辑回归、线性回归、朴素贝叶斯分类、决策树等；协同过滤实现了交替最小二乘法（Alternating Least Square，ALS）；聚类实现了K-means、高斯混合（Gaussian mixture）、Power Iteration Clustering（PIC）、Latent Dirichlet Allocation（LDA）和Streaming版本的K-means；降维实现了Singular Value Decomposition（SVD）和Principal ComponentAnalysis（PCA）；频繁模式挖掘（frequent pattern mining）实现了FP-growth。
自从Spark 1.0版本的Spark SQL问世以来，它最常见的用途之一就是作为一个从Spark平台获取数据的渠道。早期用户比较喜爱Spark SQL提供的从现有Apache Hive表以及流行的Parquet列式存储格式中读取数据的支持。之后，Spark SQL还增加了对其他格式的支持，比如说JSON。到了Spark1.2版本，Spark的原生资源与更多的输入源进行整合集成，这些新的整合将随着纳入新的Spark SQL数据源API而成为可能。Spark SQL支持的数据源如图1-6所示。
数据源API通过Spark SQL提供了访问结构化数据的可插拔机制。这使数据源有了简便的途径进行数据转换并加入到Spark 平台中。由API提供的密集优化器集合意味着过滤和列修剪在很多情况下都会被运用于数据源。这些综合的优化极大地减少了需要处理的数据量，因此能够显著提高Spark的工作效率。
Spark GraphX是Spark提供的关于图和图并行计算的API，它集ETL、试探性分析和迭代式的图计算于一体，并且在不失灵活性、易用性和容错性的前提下获得了很好的性能。现在GraphX已经提供了很多的算法，新的算法也在不断加入，而且很多的算法都是由Spark的用户贡献的。
Spark Core是Spark的核心，它体现了Spark的核心设计思想。SparkSQL、MLlib、GraphX和Spark Streaming都是基于Spark Core的实现和衍生。
##第三章RDD实现详解
RDD是Spark最基本也是最根本的数据抽象，它具备像MapReduce等数据流模型的容错性，并且允许开发人员在大型集群上执行基于内存的计算。现有的数据流系统对两种应用的处理并不高效：一是迭代式算法，这在图应用和机器学习领域很常见；二是交互式数据挖掘工具。这两种情况下，将数据保存在内存中能够极大地提高性能。为了有效地实现容错，RDD提供了一种高度受限的共享内存，即RDD是只读的，并且只能通过其他RDD上的批量操作来创建。尽管如此，RDD仍然足以表示很多类型的计算，包括MapReduce和专用的迭代编程模型（如Pregel）等。Spark实现的RDD在迭代计算方面比Hadoop快20多倍，同时还可以在5～7秒内交互式地查询1TB数据集。
在这些特性中，最难实现的是容错性。一般来说，分布式数据集的容错性有两种方式：数据检查点和记录数据的更新。我们面向的是大规模数据分析，数据检查点操作成本很高：需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低得多，同时还需要消耗更多的存储资源（在内存中复制数据可以减少需要缓存的数据量，而存储到磁盘则会降低应用程序速度）。所以，我们选择记录更新的方式。但是，如果更新太多，记录更新成本也不低。因此，RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列转换记录下来（即Lineage），以便恢复丢失的分区。
什么是RDD？RDD是只读的、分区记录的集合。RDD只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作来创建。这些确定性操作称为转换，如map、filter、groupBy、join。RDD不需要物化。RDD含有如何从其他RDD衍生（即计算）出本RDD的相关信息（即Lineage），因此在RDD部分分区数据丢失的时候可以从物理存储的数据计算出相应的RDD分区。
RDD支持基于工作集的应用，同时具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。
每个RDD有5个主要的属性：1）一组分片（Partition），即数据集的基本组成单位。2）一个计算每个分区的函数3）RDD之间的依赖关系4）一个Partitioner，即RDD的分片函数5）一个列表，存储存取每个Partition的优先位置（preferred location）
可以通过两种方式创建RDD：1）由一个已经存在的Scala集合创建。2）由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集
RDD创建后，就可以在RDD上进行数据处理。RDD支持两种操作：转换（trans-formation），即从现有的数据集创建一个新的数据集；动作（action），即在数据集上进行计算后，返回一个值给Driver程序
RDD中的所有转换都是惰性的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这个设计让Spark更加有效率地运行。
在groupByKey的操作中，会在MapPartitionsRDD做一次Shuffle，图3-3中设置的分片数量是3，因此ShuffledRDD会有3个分片，ShuffledRDD实际上仅仅是从上游的任务中读取Shuffle的结果，因此图的箭头是指向上游的MapPartitionsRDD的。
默认情况下，每一个转换过的RDD都会在它执行一个动作时被重新计算。不过也可以使用persist（或者cache）方法，在内存中持久化一个RDD。在这种情况下， Spark将会在集群中保存相关元素，下次查询这个RDD时能更快访问它。也支持在磁盘上持久化数据集，或在集群间复制数据集，这些选项将在下一节进行描述。
RDD的缓存能够在第一次计算完成后，将计算结果保存到内存、本地文件系统或者Tachyon中。通过缓存，Spark避免了RDD上的重复计算，能够极大地提升计算速度。但是，如果缓存丢失了，则需要重新计算。如果计算特别复杂或者计算耗时特别多，那么缓存丢失对于整个Job的影响是不容忽视的。为了避免缓存丢失重新计算带来的开销，Spark又引入了检查点（checkpoint）机制。
缓存是在计算结束后，直接将计算结果通过用户定义的存储级别（存储级别定义了缓存存储的介质，现在支持内存、本地文件系统和Tachyon）写入不同的介质。而检查点不同，它是在计算完成后，重新建立一个Job来计算。为了避免重复计算，推荐先将RDD缓存，这样就能保证检查点的操作可以快速完成。
file和counts都是RDD，其中file是从HDFS上读取文件并创建了RDD，而counts是在file的基础上通过flatMap、map和reduceByKey这三个RDD转换生成的。最后，counts调用了动作saveAsTextFile，用户的计算逻辑就从这里开始提交的集群进行计算。
RDD之间的关系可以从两个维度来理解：一个是RDD是从哪些RDD转换而来，也就是RDD的parent RDD(s)是什么；还有就是依赖于parent RDD(s)的哪些Partition(s)。这个关系，就是RDD之间的依赖，org.apache.spark.Dependency。根据依赖于parent RDD(s)的Partitions的不同情况，Spark将这种依赖分为两种，一种是宽依赖，一种是窄依赖。
1）窄依赖指的是每一个parent RDD的Partition最多被子RDD的一个Partition使用2）宽依赖指的是多个子RDD的Partition会依赖同一个parent RDD的Partition
对于map和filter形式的转换来说，它们只是将Partition的数据根据转换的规则进行转化，并不涉及其他的处理，可以简单地认为只是将数据从一个形式转换到另一个形式。对于union，只是将多个RDD合并成一个，parentRDD的Partition(s)不会有任何的变化，可以认为只是把parent RDD的Partition(s)简单进行复制与合并。对于join，如果每个Partition仅仅和已知的、特定的Partition进行join，那么这个依赖关系也是窄依赖。对于这种有规则的数据的join，并不会引入昂贵的Shuffle。对于窄依赖，由于RDD每个Partition依赖固定数量的parent RDD(s)的Partition(s)，因此可以通过一个计算任务来处理这些Partition，并且这些Partition相互独立，这些计算任务也就可以并行执行了。
对于groupByKey，子RDD的所有Partition(s)会依赖于parent RDD的所有Partition(s)，子RDD的Partition是parent RDD的所有Partition Shuffle的结果，因此这两个R D D是不能通过一个计算任务来完成的。同样，对于需要parent RDD的所有Partition进行join的转换，也是需要Shuffle，这类join的依赖就是宽依赖而不是前面提到的窄依赖了。
原始的RDD(s)通过一系列转换就形成了DAG。RDD之间的依赖关系，包含了RDD由哪些Parent RDD(s)转换而来和它依赖parent RDD(s)的哪些Partitions，是DAG的重要属性。借助这些依赖关系，DAG可以认为这些RDD之间形成了Lineage （血统）。借助Lineage，能保证一个RDD被计算前，它所依赖的parent RDD都已经完成了计算；同时也实现了RDD的容错性，即如果一个RDD的部分或者全部的计算结果丢失了，那么就需要重新计算这部分丢失的数据。
那么Spark是如何根据DAG来生成计算任务呢？首先，根据依赖关系的不同将DAG划分为不同的阶段（Stage）。对于窄依赖，由于Partition依赖关系的确定性， Partition的转换处理就可以在同一个线程里完成，窄依赖被Spark划分到同一个执行阶段；对于宽依赖，由于Shuffle的存在，只能在parent RDD(s)Shuffle处理完成后，才能开始接下来的计算，因此宽依赖就是Spark划分Stage的依据，即Spark根据宽依赖将DAG划分为不同的Stage。在一个Stage内部，每个Partition都会被分配一个计算任务（Task），这些Task是可以并行执行的。Stage之间根据依赖关系变成了一个大粒度的DAG，这个DAG的执行顺序也是从前向后的。也就是说，Stage只有在它没有parent Stage或者parent Stage都已经执行完成后，才可以执行。
如果存储级别不是NONE，那么先检查是否有缓存；没有缓存则要进行计算。什么是存储级别？从用户的角度来看就是缓存保存到不同的存储位置，比如内存、硬盘、Tachyon；还有缓存的数据是否需要序列化等
cacheManager对Storage模块进行了封装，使得RDD可以更加简单地从Storage模块读取或者写入数据。RDD的每个Partition对应Storage模块的一个Block，只不过Block是Partition经过处理后的数据。在系统实现的层面上，可以认为Partition和Block是一一对应的。cacheManager会通过getOrCompute来判断当前的RDD是否需要进行计算。
首先，cacheManager会通过RDD的ID和当前计算的Partition的ID向Storage模块的BlockManager发起查询请求，如果能够获得Block的信息，会直接返回Block的信息。否则，代表该RDD是需要计算的。这个RDD以前可能计算过并且被存储到了内存中，但是后来由于内存紧张，这部分内存被清理了。在计算结束后，计算结果会根据用户定义的存储级别，写入BlockManager中。这样，下次就可以不经过计算而直接读取该RDD的计算结果了
在缓存没有命中的情况下，首先会判断是否保存了RDD的checkpoint，如果有，则读取checkpoint。为了理解checkpoint的RDD是如何读取计算结果的，需要先看一下checkpoint的数据是如何写入的。首先在Job结束后，会判断是否需要checkpoint。如果需要，就调用org.apache. spark.rdd.RDDCheckpointData#doCheckpoint。doCheckpoint首先为数据创建一个目录；然后启动一个新的Job来计算，并且将计算结果写入新创建的目录；接着创建一个org.apache.spark.rdd.CheckpointRDD；最后，原始RDD的所有依赖被清除，这就意味着RDD的转换的计算链（compute chain）等信息都被清除。这个处理逻辑中，数据写入的实现在org.apache.spark.rdd.CheckpointRDD$#writeToFile。
RDD的checkpoint完成，其中checkpoint的数据可以通过checkpointRDD的readFromFile读取。但是，上述逻辑在清除了RDD的依赖后，并没有和check-pointRDD建立联系，那么Spark是如何确定一个RDD是否被checkpoint了，而且正确读取checkpoint的数据呢？答案就在org.apache.spark.rdd.RDD#dependencies的实现，它会首先判断当前的RDD是否已经Checkpoint过，如果有，那么RDD的依赖就变成了对应的CheckpointRDD
理解了Checkpoint的实现过程，接下来看一下computeOrReadCheckpoint的实现。前面提到了，它一共在两个地方被调用，org.apache.spark.rdd.RDD#iterator和org.apache.spark.CacheManager#getOrCompute。它实现的逻辑比较简单，首先检查当前RDD是否被Checkpoint过，如果有，读取Checkpoint的数据；否则开始计算。
RDD实现了基于Lineage的容错机制。RDD的转换关系，构成了computechain，可以把这个compute chain认为是RDD之间演化的Lineage。在部分计算结果丢失时，只需要根据这个Lineage重算即可。
假如RDD2所在的计算作业先计算的话，那么计算完成后RDD1的结果就会被缓存起来。缓存起来的结果会被后续的计算使用。图中的示意是说RDD1的Partition2缓存丢失。如果现在计算RDD3所在的作业，那么它所依赖的Partition0、1、3和4的缓存都是可以使用的，无须再次计算。但是Partition2由于缓存丢失，需要从头开始计算，Spark会从RDD0的Partition2开始，重新开始计算。内部实现上，DAG被Spark划分为不同的Stage，Stage之间的依赖关系可以认为就是Lineage。
##第四章scheduler模块详解
任务调度模块主要包含两大部分，即DAGScheduler和TaskScheduler，它们负责将用户提交的计算任务按照DAG划分为不同的阶段并且将不同阶段的计算任务提交到集群进行最终的计算。整个过程可以使用图4-1表示。RDD Objects可以理解为用户实际代码中创建的RDD，这些代码逻辑上组成了一个DAG，支持复杂拓扑。Spark的易用性就体现在这部分，它提供了基于RDD的多种转换和动作，使Spark的用户可以在基本不增加用户学习成本的前提下使用比较复杂的拓扑来实现策略。用户实际编程的时候可以认为它处理的数据都可以存到内存中，而无须关心最终在集群中运行的任务是否整个数据可以装载到内存中或者说究竟需要多少节点参与运算
DAGScheduler主要负责分析用户提交的应用，并根据计算任务的依赖关系建立DAG，然后将DAG划分为不同的Stage（阶段），其中每个Stage由可以并发执行的一组Task构成，这些Task的执行逻辑完全相同，只是作用于不同的数据。而且DAG在不同的资源管理框架（即部署方式，包括Standalone、Mesos、YARN、Local、EC2等）下的实现是相同的。
在DAGScheduler将这组Task划分完成后，会将这组Task提交到TaskScheduler。TaskScheduler通 过Cluster Manager在 集 群 中 的 某个Worker的Executor上 启 动任务。在Executor中运行的任务，如果缓存中没有计算结果，那么就需要开始计算，同时，计算的结果会回传到Driver或者保存在本地。在不同的资源管理框架下， TaskScheduler的实现方式是有差别的，但是最重要的实现是org.apache.spark.scheduler.TaskSchedulerImpl。对于Local、Standalone和Mesos来说，它们的TaskScheduler就是TaskSchedulerImpl；对于YARN Cluster和YARNClient的TaskScheduler的实现也是继承自TaskSchedulerImpl。
任务调度模块涉及的最重要的三个类是：1）org.apache.spark.scheduler.DAGScheduler2）org.apache.spark.scheduler.SchedulerBackend3）org.apache.spark.scheduler.TaskScheduler
其中1就是前面提到的DAGScheduler的实现。org.apache.spark.scheduler.SchedulerBackend是一个trait，作用是分配当前可用的资源，具体就是向当前等待分配计算资源的Task分配计算资源（即Executor），并且在分配的Executor上启动Task，完成计算的调度过程。它使用reviveOffers完成上述的任务调度。reviveOffers可以说是它最重要的实现。org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend是SchedulerBackend的一个实现，同时YARN、Standalone和Mesos都是基于它加入了自身特有的逻辑。
org.apache.spark.scheduler.TaskScheduler也是一个trait，它的作用是为创建它的SparkContext调度任务，即从DAGScheduler接收不同Stage的任务，并且向集群提交这些任务，并为执行特别慢的任务启动备份任务。TaskScheduler是以后实现多种任务调度器的基础，不过当前的org.apache.spark.scheduler.TaskSchedulerImpl是唯一实现。TaskSchedulerImpl会在以下几种场景下调用org.apache.spark.scheduler.Scheduler-Backend#reviveOffers：1）有新任务提交时。2）有任务执行失败时。3）计算节点（即Executor）不可用时。4）某些任务执行过慢而需要为它重新分配资源时。
前面提到过，DAGScheduler主要负责将用户的应用的DAG划分为不同的Stage （阶段），其中每个Stage由可以并发执行的一组Task构成，这些Task的执行逻辑完全相同，只是作用于不同的数据。而且DAG在不同的资源管理框架（即部署方式，包括Standalone，Mesos，YARN，Local，EC2等）下的实现是相同的。
TaskScheduler和DAGScheduler都是在SparkContext创建的时候创建的。其中， Taskcheduler是通过org.apache.spark.SparkContext#createTaskScheduler创建的。而DAGScheduler是直接调用的它的构造函数创建。只不过，DAGScheduler保存了TaskScheduler的引用，因此需要在TaskScheduler创建之后进行创建。
用户提交的Job最终会调用DAGScheduler的runJob，它又会调用submitJob。JobWaiter会监听Job的执行状态，而Job是由多个Task组成的，因此只有Job的所有Task都成功完成，Job才标记为成功；任意一个Task失败都会标记该Job失败，这是DAGScheduler通过调用org.apache.spark.scheduler.JobWaiter#jobFailed实现的。对于近似估计的Job，DAGScheduler会调用runApproximateJob，除了JobWaiter换成了org.apache.spark.partial.ApproximateActionListener，其余的逻辑是相同的。最后，DAGScheduler会向eventProcessActor提交该Job
org.apache.spark.scheduler.DAGScheduler#handleJobSubmitted首先会根据RDD创建finalStage。finalStage，顾名思义，就是最后的那个Stage。
用户提交的计算任务是一个由RDD构成的DAG，如果RDD在转换的时候需要做Shuffle，那么这个Shuffle的过程就将这个DAG分为了不同的阶段（即Stage）。由于Shuffle的存在，不同的Stage是不能并行计算的，因为后面Stage的计算需要前面Stage的Shuffle的结果。而一个Stage由一组完全独立的计算任务（即Task）组成，每个Task的运算逻辑完全相同，只不过每个Task都会处理其所对应的Partition。其中， Partition的数量和Task的数量是一致的，即一个Partition会被该Stage的一个Task处理
这个Job如果满足以下所有条件，那么它将以本地模式运行：1）spark.localExecution.enabled设置为true2）用户程序显式指定可以本地运行3）finalStage没有parent Stage4）仅有一个Partition
org.apache.spark.scheduler.DAGScheduler#submitMissingTasks会完成DAGScheduler最后的工作，向TaskScheduler提交Task。首先，它先取得需要计算的Partition，对于最后的Stage，它对应的Task是ResultTask，因此判断该Partition的ResultTask是否已经结束，如果结束那么就无需计算；对于其他的Stage，它们对应的Task都是ShuffleMapTask，因此只需要判断Stage是否有缓存的结果即可
每 个TaskScheduler都 对 应 一 个SchedulerBackend。其 中，TaskScheduler负 责Application的不同Job之间的调度，在Task执行失败时启动重试机制，并且为执行速度慢的Task启动备份的任务。而SchedulerBackend负责与Cluster Manager交互，取得该Application分配到的资源，并且将这些资源传给TaskScheduler，由TaskScheduler为Task最终分配计算资源。
由submitTasks开始Task级别的资源调度。最终，这些Task会被分配Executor，运行在Worker上的Executor完成任务的最终执行。
TaskSetManager会根据数据的就近原则（locality aware）为Task分配计算资源，监控Ta s k的执行状态并采取必要的措施，比如失败重试，慢任务的推测性执行。
schedulableBuilder是Application级别的调度器，现在支持两种调度策略，FIFO （Fist In First Out，先进先出）和FAIR（公平调度）。调度策略可以通过spark.scheduler. mode设置，默认是FIFO。schedulableBuilder会确定TaskSetManager的调度顺序，然后由TaskSetManager根据就近原则来确定Task运行在哪个Executor上。
1.Driver收到Executor的任务执行结果Task在Executor执行完成时，会通过向Driver发送StatusUpdate的消息来通知Driver任务的状态更新为TaskState.FINISHED。Driver首先会将任务的状态更新通知TaskScheduler，然后会在这个Executor上重新分配新的计算任务。TaskScheduler的实现在org.apache.spark.scheduler.Task-SchedulerImpl#statusUpdate
一个Task的状态只有是TaskState.FINISHED才标记它成功执行；其余的状态包括TaskState.FAILED、TaskState.KILLED和TaskState.LOST都是执行失败。其中，处理每次结果都是由一个Daemon线程池负责，默认这个线程池由4个线程组成，可以通过spark.resultGetter.threads设置
Executor在将结果回传到Driver时，会根据结果的大小使用不同的策略：1）如果结果大于1GB，那么直接丢弃这个结果。这个是Spark1.2中新加的策略。可以通过spark.driver.maxResultSize来进行设置。2）对于“较大”的结果，将其以tid为key存入org.apache.spark.storage.Block-Manager；如果结果不大，则直接回传给Driver。那么如何判定这个阈值呢？这里的回传是直接通过AKKA的消息传递机制。因此这个大小首先不能超过这个机制设置的消息的最大值。这个最大值是通过spark.akka.frameSize设置的，单位是MByte，默认值是10MB。除此之外，还有200KB的预留空间。因此这个阈值就是conf.getInt(""spark.akka.frameSize"，10)*1024*1024-200*1024。3）其他的直接通过AKKA回传到Driver。
##第五章deploy模块详解
Spark的Cluster Manager有以下几种部署模式：Standalone，Mesos，YARN，EC2， Local。Spark的Cluster Manager最开始的时候仅支持Mesos，后来为了方便Hadoop的用户加入了对YARN的支持，即使用YARN作为资源的管理器和应用内的任务调度器。而Standalone模式的出现，更好地扩展了Spark的应用场景。本章将着重讲解Standalone部署方式的详细实现，即Deploy模块的详细实现。
在4.3.1节介绍TaskScheduler的创建时，提到过不同的运行模式实际上实现了不同的SchedulerBackend和TaskScheduler：1）org.apache.spark.scheduler.SchedulerBackend2）org.apache.spark.scheduler.TaskScheduler在SparkContext的创建过程中，会通过传入的Master URL的值来确定不同的运行模式，并且创建不同的SchedulerBackend和TaskScheduler，这个创建过程的实现在org.apache.spark.SparkContext#createTaskScheduler。
Deploy模块采用的也是典型的Master/Slave架构，其中Master负责整个集群的资源调度和Application的管理。Slave（即Worker）接收Master的资源分配调度命令后启动Executor，由Executor完成最终的计算任务。而Client则负责Application的创建和向Master注册Application，并且负责接收来自Executor的状态更新和计算结果等。
Deploy模块主要包含3个子模块：Master、Worker、Client，它们之间的通信通过AKKA完成。对于Master和Worker，它们本身就是一个Actor，因此可以直接通过AKKA实现通信。Client虽然本身不是一个Actor，这三者的主要职责如下：1）Master：接收Worker的注册并管理所有的Worker，接收Client提交的Application， FIFO调度等待的Application并向Worker提交。2）Worker：向Master注册自己，根据Master发送的Application配置进程环境，并启动StandaloneExecutorBackend。3）Client：向Master注册并监控Application。当用户创建SparkContext时会实例化SparkDeploySchedulerBackend，而实例化SparkDeploySchedulerBackend的同时会启动Client，通过向Client传递启动参数和Application有关信息，Client向Master发送请求注册Application并且在计算节点上启动StandaloneExecutorBackend。
Master作为整个集群的管理者，需要Worker通过注册、汇报状态来维护整个集群的运行状态，并且通过这些状态来决定资源调度策略等。
Worker向Master发送的消息主要包含三类：1）注册：Worker启动时需要向Master注册，注册时需要汇报自身的信息。2）状态汇报：汇报Executor和Driver的运行状态；在Master故障恢复时，需要汇报Worker上当前运行的Executor和Driver的信息。3）报活心跳：Worker每隔指定周期会向Master发送报活的心跳。
Master向Worker发送的消息除了响应Worker的注册外，还有一些控制命令，包括让Worker重新注册、让Worker启动Executor或者Driver、停止Executor和Driver等。
这里的Client分为Driver Client和AppClient。
org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverActor负责，而Executor实际上对应于org.apache.spark.executor.CoarseGrainedExecutorBackend。Driver向Executor发送的消息分为两类：1）启动Task，停止Task。2）响应Executor注册的请求，回复成功还是失败。
Master的 实 现 是org.apache.spark.deploy.master.Master。它 是 一 个Actor，因此Worker和AppClient可以直接和它通过AKKA进行通信。一个集群可以部署多个Master，以达到高可用性的目的，因此它还实现了org.apache.spark.deploy.master.LeaderElectable以在多个Master中选举出一个Leader。
除了选举机制，还要注意的是Master元数据持久化的方式。Master保存了整个集群的元数据，包含Worker、Application和Driver Client。Spark的Standalone模式支持以下几种方式的元数据持久化方式和选举机制：1.ZooKeeper2.FILESYSTEM3.CUSTOM4.NONE
在Master的进程启动后，ZooKeeper方式的选举机制会根据自身策略来选举出Leader；对于FILESYSTEM和NONE方式，进程启动后会立即成为Leader。这通过调用org.apache.spark.deploy.master.Master#electedLeader
而被选举为Leader的Master，会首先读取集群的元数据信息，如果有读到的数据，那么Master的状态就会变为RecoveryState.RECOVERING，然后开始恢复数据和通知Worker、AppClient和Driver Client，Master已经更改，恢复结束后Master的状态会变成RecoveryState.ALIVE。对于没有读取任何数据的Master，状态会立即变成RecoveryState.ALIVE。Master只有在状态是RecoveryState.ALIVE时才可以对外服务，包括接受Worker、Application和Driver Client的注册和状态更新等。
对于已经有Application运行的集群来说，Master故障恢复的时候，是需要将Application、Worker和Driver Client的元数据恢复的。恢复数据通过调用begin-Recovery实现。
Master在接到AppClient和Worker的响应后，会将它们的状态从UNKNOWN设置为正常的状态。对于Driver Client，只有在分配给的Worker出现问题时才会重新调度这个Driver Client。因此这里的恢复只是将它加到Master在内存中维护的DriverClient列表中。
由于这里的恢复只是以异步的方式通知AppClient和Worker，那么什么时候才会结束呢？首先，Master在接到AppClient的响应（消息MasterChangeAcknowledged）和Worker的响应（消息WorkerSchedulerStateResponse）后，会查看是不是所有的Application和Worker的状态都不是UNKNOWN了。如果确认都不是，则代表恢复已经完成。若有部分AppClient或者Worker确实出了问题，长时间没有响应呢？Master还有第二个机制，即设置一个超时时间，如果超时之后仍有AppClient或者Worker未响应，那么Master还是会认为恢复已经结束。默认超时时间为60秒。对于规模不同的集群，这个超时时间可以通过spark.worker.timeout来设置，单位是秒。
Worker的启动相对于Master的启动要简单很多。Worker启动只会做一件事情，就是向Master注册。在接到Worker的注册请求后，如果Master是Active的并且Worker没有注册过，那么Master会回复Worker 消息RegisterWorker，表示Worker注册成功；若注册失败，Master会回复消息RegisterWorkerFailed，Worker接到该消息后直接退出（由于Worker会重复多次发送请求，因此退出前需要判断是否注册成功了，如果没有注册成功才会退出；如果已经注册成功了，那么忽略这个消息）。
Worker在向Master注册的时候有重试机制，即在指定时间如果收不到Master的响应，那么Worker会重新发送注册请求。目前重试的次数至多为16次。为了避免所有的Worker都在同一个时刻向Master发送注册请求，每次重试的时间间隔是随机的。而且前6次的重试间隔在5～15秒，而后10次的重试间隔在30～90秒。在Worker刚启动时，会调用org.apache.spark.deploy.worker.Worker#registerWithMaster来进行注册。第一次调用registerWithMaster的时候除了向所有的Master发出注册请求，还要启动一个定时器，在超时的时候进行重新注册。
容错（fault tolerance）指的是在一个系统的部分模块出现错误的情况下还能持续地对外提供服务；如果出现了服务质量的下降，这个下降也是和出错的严重性成正比的。对于没有容错的系统，即使一个微小的错误也可能会导致整个服务停止。
提到容错，不得不提一下容灾（或者称为灾难恢复，disaster recovery）。容灾技术是通过在异地建立和维护一个备份系统，利用地理上的分散性来保证数据对于灾难性事件的抵抗能力。容灾系统在实现中可以分为两个层次：数据容灾和应用容灾。数据容灾指建立一个异地的数据系统，作为本地关键应用数据的一个备份。应用容灾是在数据容灾的基础上，在异地建立一套完整的和本地生产系统相同的备份应用系统（可以是互为备份）。在灾难情况下，由远程系统迅速接管业务运行。
容错和容灾都是为了实现系统的高可用性，容错是在系统的部分模块出现问题时的错误恢复机制；容灾则是在整个系统的层面，通过使用数据和应用的镜像，来实现服务的高可用性的。
对于一个集群来说，Worker的异常退出发生概率非常高。Worker退出时，集群是如何进行容错处理的呢？1）Worker在退出前，会将所有运行在它上面的Executor和Driver Client删除。2）Worker需要周期性地向Master发送心跳消息。这个周期就是spark.worker.timeout （默认值60秒）设置的1/4。由于Worker的异常退出，使得它的心跳会超时，Master认为该Worker已经异常退出，那么Master会将该Worker上运行的所有Executor的状态标记为丢失（ExecutorState.LOST），然后将这个状态更新通过消息ExecutorUpdated通知AppClient；对于该Worker上运行的DriverClient，如果它设置了需要重启（即设置了supervise），那么需要重新调度来重新启动这个Driver Client，否则直接将它删除，并且将状态设置为DriverState.ERROR。上述逻辑的实现在类org.apache.spark.deploy.master.Master的成员removeWorker中，感兴趣的读者可以通过代码来加深理解。3）AppClient接到Master的StatusUpdate消息后会将状态更新汇报到org.apache. spark.scheduler.cluster.SparkDeploySchedulerBackend，而它会根据消息内容来判断是否是Executor异常退出。
Worker在退出的时候，会通过org.apache.spark.deploy.worker.ExecutorRunner杀死Executor。ExecutorRunner实际上会将以进程的方式启动org.apache.spark.executor. CoarseGrained-ExecutorBackend。这个启动过程的实现在fetchAndRunExecutor。因此，Executor-Runner杀死Executor实际上就是杀死CoarseGrainedExecutorBackend的过程。
Executor模块负责运行Task计算任务，并将计算结果回传到Driver。Spark支持多种资源调度框架，这些资源框架在为计算任务分配了资源后，最后都会使用Executor模块完成最终的计算。在讲解Executor异常退出的容错机制前，需要先了解Executor的启动机制。Worker接收到Master的LaunchExecutor命令后，会创建org.apache.spark.deploy.worker.Executor-Runner。org.apache.spark.deploy.worker.ExecutorRunner#fetchAndRunExecutor会负责Driver通过AKKA通信。它在启动后，会首先向Driver注册Executor
##第六章executor模块详解
Executor模块负责运行Task计算任务，并将计算结果回传到Driver。Spark支持多种资源调度框架，这些资源框架在为计算任务分配资源后，最后都会使用Executor模块完成最终的计算。
每个Spark的Application都是从Spark-Context开始的，它通过Cluster Manager和Worker上的Executor建立联系，由每个Executor完成Application的部分计算任务。不同的Cluster Master，即资源调度框架的实现模式会有区别，但是任务的划分和调度都是由运行SparkContext端的Driver完成的，资源调度框架在为Application分配资源后，将Task分配到计算的物理单元Executor去处理。本章将着重讲解Executor模块的实现。
AppClient会回调相关函数以通知SparkDeploySchedulerBackend：1）向Master成功注册Application，即成功连接到集群。2）断开连接，如果当前SparkDeploySchedulerBackend::stop ==false，那么可能原来的Master有故障了，待新的Master准备就绪后，会重新恢复原来的连接。3）Application由于不可恢复的错误停止了，这个时候需要重新提交出错的TaskSet。4）添加一个Executor，在这里仅仅实现了打印log，并没有额外的逻辑。5）删除一个Executor，可能有两个原因：一个是Executor退出了，这里可以得到Executor的退出码；另一个是由于Worker的退出导致运行在它之上的Executor退出。这两种情况需要不同的逻辑来处理
为Application分配资源选择Worker（Executor），现在有两种策略：1）尽量打散，即将一个Application尽可能多地分配到不同的节点。可以通过设置spark.deploy.spreadOut来实现。默认值为true。2）尽量集中，即一个Application尽量分配到尽可能少的节点。CPU密集型而内存占用比较少的Application适合使用这种策略。
Executor的内存是被其内部所有的任务共享，而每个Executor上可以支持的任务的数量取决于Executor所持有的CPU Core的数量。因此为了评估一个Executor占用多少内存是合适的，需要了解每个任务的数据规模的大小和计算过程中所需要的临时内存空间的大小。实际上，要比较精确地计算出一个任务所需要的内存空间还是非常困难的，首先，因为数据本身加载到内存中，由于有管理这些内存的额外内存开销，可能需要的真实内存是数据大小的数倍；其次，任务计算过程中所需要的临时内存空间的大小会因为算法的不同而不同，这是比较难评估的。如果需要比较准确的评估数据集的大小的话，可以将RDD cache在内存中，从BlockManager的日志中可以看到每个Cache分区的大小（实际上，这个大小也是一个估计值）。如果内存比较紧张，就需要合理规划每个分区任务的数据规模，例如采用更多的分区，用增加任务数量（进而需要更多的批次来运算所有的任务）的方式来减小每个任务所需处理的数据大小。
本章以Standalone模式为例，解析用户提交的应用是如何分配到Executor资源的，接着详细介绍了Task在Executor执行的实现细节。
###shuffle模块详解
Shuffle，无疑是性能调优的一个重点，本章将从源码实现的角度，深入解析Spark Shuffle的实现细节；并且根据Shuffle实现迭代的历史，了解Spark的Shuffle如何解决遇到的问题，这个对于互联网工程实践也具有很重要的指导意义：一个可用、可上线与完美的方案是有区别的，与其自己在线下不断“优化”，不如将一个能够满足当前需要的实现先上线，并且根据需求或者应用场景的变化不断进行迭代、改进。这样，一个系统或者功能就会慢慢地发展起来。
Shuffle，翻译成中文就是洗牌。之所以需要Shuffle，还是因为具有某种共同特征的一类数据需要最终汇聚（aggregate）到一个计算节点上进行计算。这些数据分布在各个存储节点上并且由不同节点的计算单元处理。以最简单的Word Count为例，其中数据保存在Node1、Node2和Node3；经过处理后，这些数据最终会汇聚到Nodea、Nodeb处理
这个数据重新打乱然后汇聚到不同节点的过程就是Shuffle。但是实际上，Shuffle过程可能会非常复杂：1）数据量会很大，比如单位为TB或PB的数据分散到几百甚至数千、数万台机器上。2）为了将这个数据汇聚到正确的节点，需要将这些数据放入正确的Partition，因为数据大小已经大于节点的内存，因此这个过程中可能会发生多次硬盘续写。3）为了节省带宽，这个数据可能需要压缩，如何在压缩率和压缩解压时间中间做一个比较好的选择？4）数据需要通过网络传输，因此数据的序列化和发序列化也变得相对复杂。
一般来说，每个Ta s k处理的数据可以完全载入内存（如果不能，可以减小每个Partition的大小），因此Task可以做到在内存中计算。除非非常复杂的计算逻辑，否则为了容错而持久化中间的数据是没有太大收益的，毕竟中间某个过程出错了可以从头开始计算。但是对于Shuffle来说，如果不持久化这个中间结果，一旦数据丢失，就需要重新计算依赖的全部RDD，因此有必要持久化这个中间结果。
这个逻辑比较简单，总结如下：1）从SparkEnv中获得shuffleManager，就如前面提到的，Spark除了支持Hash和Sort Based Shuffle外，还支持external 的Shuffle Service。用户可以通过实现几个类就可以使用自定义的Shuffle。2）从manager里取得Writer，在这里获得的是org.apache.spark.shuffle.hash. HashShuffleWriter。3）调用rdd开始运算，运算结果通过Writer进行持久化，逻辑在org.apache. spark.shuffle.hash.HashShuffleWriter#write。开 始 时 通过org.apache.spark.Shuffle-Dependency是否定义了org.apache.spark.Aggregator来确定是否需要做Map端的聚合。然后将原始结果或者聚合后的结果通过org.apache.spark.shuffle.FileShuffleBlockManager#forMapTask的方法写入。写入完成后，会将元数据信息写入org.apache. spark.scheduler.MapStatus。然后下游的Task可以通过这个MapStatus取得需要处理的数据。
由于每个Shuffle Map Task需要为每个下游的Task创建一个单独的文件，因此文件的数量就是number( shuffle_map_task) * number(following_task)。如果Shuffle Map Task是1000，下游的Task是500，那么理论上会产生500 000个文件（对于size 为0的文件Spark有特殊的处理）。生产环境中Task的数量实际上会更多，因此这个简单的实现会带来以下问题：1）每个节点可能会同时打开多个文件，每次打开文件都会占用一定内存。假设每个Write Handler的默认需要100KB的内存，那么同时打开这些文件需要50GB的内存，对于一个集群来说，还是有一定的压力的。尤其是如果Shuffle Map Task和下游的Task同时增大10倍，那么整体的内存就增长到5TB。2）从整体的角度来看，打开多个文件对于系统来说意味着随机读，尤其是每个文件比较小但是数量非常多的情况。而现在机械硬盘在随机读方面的性能特别差，非常容易成为性能的瓶颈。如果集群依赖的是固态硬盘，也许情况会改善很多，但是随机写的性能肯定不如顺序写的。
##第八章storage模块详解
Storage模块负责管理Spark计算过程中产生的数据，包括基于Disk的和基于Memory的。用户在实际编程中，面对的是RDD，可以将RDD的数据通过调用org. apache.spark.rdd.RDD#cache将数据持久化；持久化的动作都是由Storage模块完成的，包括Shuffle过程中的数据，也都是由Storage模块管理的。可以说，RDD实现用户的逻辑，而Storage管理用户的数据。在Driver端和Executor端，都会有Storage模块，那么它们功能的共同点和不同点是什么？本章将讲解Storage模块的实现。
Storage模块采用的是Master/Slave的架构。Master负责整个Application的Block的元数据信息的管理和维护；而Slave需要将Block的更新等状态上报到Master，同时接收Master的命令，比如删除一个RDD、Shuffle相关的数据或者是广播变量。而Master与Slave之间通过AKKA消息传递机制进行通信。在SparkContext创建时，它会创建Driver端的SparkEnv，而SparkEnv会创建BlockManager，BlockManager创 建 的 时 候 会 持 有 一 个BlockManagerMaster。BlockManagerMaster会把请求转发给BlockManagerMasterActor来完成元数据的管理和维护。
而在Executor端，也存在一个BlockManager，它也会持有一个BlockManager-Master，只不过BlockManagerMaster会持有一个Driver端BlockManagerMasterActor的Reference，因此Executor端的BlockManager就能通过这个Actor的Reference将Block的信息上报给Master。BlockManager本身还持有一个BlockManagerSlaveActor，而这个Slave的Actor还会被上报到Master。Master会持有这个Slave Actor的Reference，并通过这个Reference向Salve发送一些命令，比如删除Slave上的RDD、Shuffle相关的数据或者是广播变量。


















